{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History of Neural Networks\n",
    "* First Conceptualizes 1943\n",
    "* First models were visualizations of perceptrions introduced in 1958 by Frank Rosenblatt\n",
    "    * Belief that artificial neural networks (ANNs) would soon be able to translate languages on the fly.\n",
    "* However, it lacked technology to actually get something to work. Period of time known as the \"AI Winter\"\n",
    "\n",
    "**ImageNet 2012**  \n",
    "Large-scale Visual Recognition Challenge. \n",
    "Reintroduced the idea of Neural Networks.\n",
    "\n",
    "$$\n",
    "y = w_0 + \\sum{w_i x_i}\n",
    "$$\n",
    "Or\n",
    "$$\n",
    "y = \\vec{w} * \\vec{x}\n",
    "$$ \n",
    "\n",
    "Multiple outputs of y:\n",
    "$$\n",
    "\\bar{Y} = W \\bar{X}\n",
    "$$\n",
    "Solving the weights:\n",
    "$$\n",
    "\\bar{W} = \\bar{Y}\\bar{X}^{+}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "#### Load Data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "#### Cross Validation (split data)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         8\n",
      "           1       0.82      0.82      0.82        11\n",
      "           2       1.00      0.82      0.90        11\n",
      "\n",
      "    accuracy                           0.87        30\n",
      "   macro avg       0.87      0.88      0.87        30\n",
      "weighted avg       0.88      0.87      0.87        30\n",
      "\n",
      "[[8 0 0]\n",
      " [2 9 0]\n",
      " [0 2 9]]\n"
     ]
    }
   ],
   "source": [
    "#### Perceptron Model\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X_train, y_train)\n",
    "\n",
    "#### Test the perceptron\n",
    "y_pred = per_clf.predict(X_test)\n",
    "\n",
    "#### Cross Validation (test data)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test above wasn't very accurate. We could do better.\n",
    "\n",
    "## Activation Functions\n",
    "The __activation__ is the computed value of a neuron (or node)\n",
    "\n",
    "For example,\n",
    "$$\n",
    "y_1 = w_0 \\sum{w_{1i} x_i}\n",
    "$$\n",
    "\n",
    "An __activation function__ can adjust the value of the output neuron, depending on its value.  \n",
    "$$\n",
    "y_1 = \\sigma \\left( w_0 \\sum{w_{1i} x_i} \\right)\n",
    "$$\n",
    "\n",
    "Activation functions are generally non-linear.\n",
    "Sometimes, the output of the activation function is a value within the range of that function n(linear activation function, or ReLu). These are great for regression networks.  \n",
    "\n",
    "Let $z = w_0 + \\sum{w_{1i} x_i}$\n",
    "\n",
    "## Good for Linear Regressions \n",
    "#### 1. Linear Activation Function\n",
    "Just a linear transformation of each y value. Takes the linear regression line and adjusts the slope.\n",
    "$$y = \\sigma(z) = \\alpha z$$\n",
    "\n",
    "#### 2. Rectified Linear Unit (ReLU)  \n",
    "The max means that we keep our linear regression above zero, but after it is above zero, we follow the liner regression line.  \n",
    "$$y = \\sigma(z) = \\max\\{{0, z}\\}$$\n",
    "\n",
    "\n",
    "## Good for Classifications\n",
    "Sometimes the output of the activation function is a value between 0 and 1. This essentially gives a probability, so it's good for classification models. \n",
    "#### 1. Step Function (aka Threshold Logic Unit (TLU))  \n",
    "Gives a straight jump from 0 to 1.\n",
    "$$ y = \\sigma = \\begin{cases}1  &\\text{if }z \\ge threshold \\\\ 0 & \\text{if } z < threshold \\end{cases} $$\n",
    "\n",
    "#### 2. Sigmoid Function (aka Logical Function)  \n",
    "Gives a gradual transition from 0 to 1.\n",
    "$$\n",
    "    y = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "#### 3. Hyperbolic Tangent  \n",
    "Acts like the sigmoid, but between -1 and 1 instead of 0 and 1.\n",
    "$$\n",
    "    y = \\sigma(z) = \\tanh{z} = \\frac{e^{z} - e^{-z}}{e^z + e^{-z}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron (Picture in notebook)\n",
    "When you have multiple inputs and multiple outputs, each having their own weight.\n",
    "\n",
    "## Types of Layers\n",
    "1. Input layer\n",
    "    - petal length, petal width, sepal length\n",
    "    - color of a pixel\n",
    "    - weight, height, \n",
    "2. Hidden Layer\n",
    "    - very complicated and not interperatbale\n",
    "3. Output layer\n",
    "    - classification of flower\n",
    "    - category of image\n",
    "    - diagnosis of cancer or not\n",
    "\n",
    "**Width** is the size of the layer. The number of *neurons* in a layer.  \n",
    "**Depth** is the number of *hidden layers*\n",
    "\n",
    "## Types of Neural Networks  \n",
    "\n",
    "**Artificial Nueral Network (ANN)** is when you have *one or more* more than one hidden layer in the perceptron.  \n",
    "**Deep Neural Network (DNN)** is when you have *two or more* hidden layers in the perceptron.  \n",
    "\n",
    "## Finding Weights\n",
    "**1. Very Simplistic Way**  \n",
    "\n",
    "If you find that your output is terrible, you can adjust the weights little by little. \n",
    "\n",
    "To measure how well your predictions are, you can do mean squared error, absolute error, or other loss funcitons.  \n",
    "\n",
    "Simple Cost function using substraction would look like this: $(- \\eta \\hat{y_i} - y_i)x_j$ where $\\eta$ is the learning rate.\n",
    "\n",
    "You can then minimize the loss function. Tells you how much you need to change the weight by:\n",
    "\n",
    "$$\n",
    "w_{ij}' = w_{ij} - \\eta (\\hat{y_i} - y_i)x_j\n",
    "$$\n",
    "\n",
    "**2. Steps for Gradient Descent**  \n",
    "1. Run the dataset through it multiple times, varying the weights each time\n",
    "2. you get an average suggested change\n",
    "3. Apply the average change\n",
    "3. Then run again until the average change is very \n",
    "\n",
    "repeat 30-40 different epocs, applying average change every time until the average change is very little. \n",
    "\n",
    "# Cautions\n",
    "1. Easy to overfit\n",
    "2. Not good for generalizing. (you might not be able to use a model you make on some other data set.)\n",
    "    - Reason: because hidden layers are not interpretable\n",
    "3. Incorporating physics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Networks using Back Propogation  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Questions to have when using NN's\n",
    "\n",
    "- How many input nodes?  \n",
    "- How many output nodes?  \n",
    "- How many hidden layers? \n",
    "    - _one hidden layer is generally enough_\n",
    "    - _but..._\n",
    "        - _more layers tend to give a higher parameter efficiency_\n",
    "        - _sometimes, two layers of 20 do more than one layer of 100. Since there are fewer nodes, it also creates a faster model_\n",
    "\n",
    "    - Deep networks use exponentially fewer nodes than shallow networks. \n",
    "    - _Lower layers_ (closer to the input layer) tend to model low-level structures (like line-segments, color, and other general information)\n",
    "    - _Intermediate layers_ (in the middle) tend to model intermediate level structures (like going from line-segments to general shapes. Going from a color to color patterns.)\n",
    "    - _Higher Layers_ (closer to the output layer) tent to model high-level structures (like going from a shape to an apple, from color patterns to color themes, specific information)  \n",
    "\n",
    "\n",
    "    Input -> Middle -> Output  \n",
    "    General ---------> Specific  \n",
    "\n",
    "\n",
    "    - _Transfer Learning_ is when Lower layers can be reused in other models. For example, you can transfer some layers to model similar, but different data.  \n",
    "\n",
    "\n",
    "- What is the width of each hidden layer?\n",
    "    - Doesn't have to really follow a pyramid shape. \n",
    "    - Equally sized layers tend to perform better than a pyramid shape.\n",
    "    - Higher the width, the more calculations it has to do. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fashion_mnist  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
