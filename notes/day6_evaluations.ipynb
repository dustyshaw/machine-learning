{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Evaluations  \n",
    "\n",
    "**Loss Function** - pretty much the same as an evaluation, except it applies the same functions in order to actually improve the model.\n",
    "\n",
    "1.  **Accuracy**: the fraction of correct predictions  \n",
    "\n",
    "$$\n",
    "\\frac{num correct}{num predicted}\n",
    "$$\n",
    "\n",
    "- the bigger your data set, the more you're getting wrong, so the accuracy doesn't really scale very well. \n",
    "\n",
    "2.  **Recall**: find all relevant cases. How many did I correctly predict?\n",
    "\n",
    "$$\n",
    "recall = \\frac{truepos}{truepos + falseneg}\n",
    "$$\n",
    "\n",
    "falseneg = It should have been positive, but I gave it a negative.  \n",
    "\n",
    "3. **Precision**\n",
    "$$\n",
    "precision = \\frac{truepos}{truepos + falsepos}\n",
    "$$\n",
    "\n",
    "4. **F1 score**: essentially the average between recall and precision (harmonic mean).  \n",
    "$$\n",
    "F_1 = 2 * \\frac{precision * recall}{precision + recall}\n",
    "$$\n",
    "\n",
    "## Confusion Matrix  \n",
    "If you have predicted it being in that category, then it would be true positive if it really is a true positive.  \n",
    "\n",
    "Pos Prediction + Actual Positive = True Pos    \n",
    "Neg Prediction + Actual Positive = False Neg (Type II Error)   \n",
    "Pos Prediction + Actual Negative = False Pos (Type I Error)  \n",
    "Neg Prediction + Actual Negative = True Neg  \n",
    "\n",
    "\n",
    "**Example** Try to classify students into different categories.  \n",
    "\n",
    "Group 0, 1, 2, 3\n",
    "| #      |1      |  2    |  3   | \n",
    "| ----- | ------- |------- |------- |\n",
    "| Actual 1 | 18 | 2 | 0 |\n",
    "| Actual 2 | 0 | 22 | 4 |\n",
    "| Actual 3 | 1 | 6 | 17 |\n",
    "\n",
    "Accuracy = 57 / 69\n",
    "\n",
    "**Group 1**  \n",
    "Precision = (18) / (18 + 1) = 0.94  \n",
    "[Column Wise]\n",
    "\n",
    "Recall = (18) / (18 + 2) = 0.90  \n",
    "\n",
    "F1_Score = 2 * (0.94 * 0.9) / (0.94 + 0.9) = \n",
    "\n",
    "\n",
    "# Quantitative Evaluations   \n",
    "Now, we predict a point $\\hat{y_i}$ of a true value $y_i$\n",
    "\n",
    "\n",
    "1. **Absolute Error**   \n",
    "\n",
    "We want a function that shows the error of our model (and Large errors cause more problems, so large errors are \"penalties\" to the model). any individual point that has a large error, we want to penalize so that we can see that mistake. \n",
    "$$\n",
    "abserror = \\sum{ | {\\hat{y_i} - y_i} |}\n",
    "$$\n",
    "\n",
    "2. **Mean Absolute Error**  \n",
    "$$\n",
    "abserror = \\frac{1}{n}\\sum{ | {\\hat{y_i} - y_i} |}\n",
    "$$  \n",
    "- Smaller than absolute error, so easier to understand per point maybe\n",
    "- Pretty easy to calculate\n",
    "- Outliers aren't weighted enough. so an average doesn't really make a red flag about outliers.\n",
    "- Penalizes all errors equally rather than penalizing very far errors more.\n",
    "\n",
    "3. **Mean Square Error (MSE)**  \n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum{  {\\hat{y_i} - y_i}^2 }\n",
    "$$  \n",
    "- But what even is a square dollar? \n",
    "\n",
    "4. **Root Mean Square Error (RMSE)**  \n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt {\\frac{1}{n}\\sum{  {\\hat{y_i} - y_i}^2 }}\n",
    "$$  \n",
    "\n",
    "- Now this applies heavy penalty to large errors, which is what we want.\n",
    "- Now it is in the units we usually think of, not Square Dollars\n",
    "\n",
    "5. **Log Error**\n",
    "\n",
    "\n",
    "# Loss Functions  \n",
    "\n",
    "Loss function uses error measures to make small adjustments of the model in order to improve it. Sees where the errors are and does a slight adjustment to minimize error.  \n",
    "Determine how far off we are, and adjust the numbers and run the model again.  \n",
    "In order to make updates...\n",
    "- Optimization (from calculus)\n",
    "    - Local minima\n",
    "    - large steps in one variable causes large steps in other variables\n",
    "- Gradient Descent (solves problem 2 - large steps)\n",
    "    - Use optimization to find what direction you need to go in, then take a small step.\n",
    "- Stochastic Gradient Descent\n",
    "    - Stochastic means random\n",
    "    - Fixes the local minima problem and large steps\n",
    "    - Add some random motion to the gradient descent.\n",
    "\n",
    "Other loss functions...  \n",
    "- All Evaluation Metrics we have talked about (precision, recall, MSE, RMSE)\n",
    "- 0-1 loss function:  classification models \n",
    "    - 0 if the model correctly predicts the classification\n",
    "    - 1 if the model incorrectly predicts\n",
    "    - Take the sum of incorrect predictions\n",
    "- Logistic Regression (s-curve sigmoid thingy)\n",
    "- Hinge function (if its way incorrect, then it penalizes more than the more correct ones.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
