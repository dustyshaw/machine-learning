{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "**January 15, 2025**\n",
    "\n",
    "To train a model, don't test it on the data used to train the model! You can't really tell how good your model is if you don't.  \n",
    "\n",
    "- Separate data into testing and training data.  \n",
    "    - 20% could be a good for testing data. But what 20% do you take?\n",
    "    - could take just the last chunk if it is random data. \n",
    "    - Or take our random chunks of data for testing data. \n",
    "\n",
    "Sometimes data is split into *three* sets:\n",
    "1. Training (70)\n",
    "2. Validation (20)\n",
    "3. Testing (10)\n",
    "\n",
    "## 1. k-fold Cross Validation  \n",
    "Separates data into k groups (called 'folds')  \n",
    "- If you want 20%, then you want 5 folds  \n",
    "Trains your model k times, setting aside 1 fold for testing each time.\n",
    "\n",
    "## 2. Leave-one-out Cross Validation (LOOCV)  \n",
    "A fancy form of k-fold where it trains your model on all data except 1 data point. (1000 data points means it will model it 1000 times)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2520.0, 1.0, 0.0, 0.0, 1.0, 10, 194.5],\n",
       "       [1850.0, 0.0, 1.0, 0.0, 3.0, 10, 193.0],\n",
       "       [1925.0, 0.0, 0.0, 1.0, 2.0, 30, 191.8],\n",
       "       [1790.0, 1.0, 0.0, 0.0, 3.0, 20, 187.0],\n",
       "       [2120.0, 0.0, 1.0, 0.0, 2.0, 10, 189.0],\n",
       "       [1910.0, 0.0, 0.0, 1.0, 2.0, 35, 186.0],\n",
       "       [1845.0, 1.0, 0.0, 0.0, 1.0, 20, 186.0],\n",
       "       [2343.0, 0.0, 1.0, 0.0, 1.0, 15, 189.0],\n",
       "       [1886.0, 0.0, 0.0, 1.0, 3.0, 30, 188.0],\n",
       "       [2149.0, 1.0, 0.0, 0.0, 3.0, 15, 190.0],\n",
       "       [1797.0, 0.0, 1.0, 0.0, 3.0, 10, 187.0],\n",
       "       [1990.6666666666667, 0.0, 0.0, 1.0, 2.0, 25, 186.0],\n",
       "       [1934.0, 1.0, 0.0, 0.0, 3.0, 10, 184.0],\n",
       "       [2129.0, 0.0, 1.0, 0.0, 1.0, 5, 186.0],\n",
       "       [1872.0, 0.0, 0.0, 1.0, 0.0, 0, 185.0],\n",
       "       [1957.0, 1.0, 0.0, 0.0, 2.0, 15, 183.5],\n",
       "       [1790.0, 0.0, 1.0, 0.0, 2.0, 10, 181.0],\n",
       "       [1990.6666666666667, 0.0, 0.0, 1.0, 3.0, 30, 180.0],\n",
       "       [1842.0, 1.0, 0.0, 0.0, 3.0, 25, 178.0],\n",
       "       [2173.0, 0.0, 1.0, 0.0, 2.0, 15, 178.3]], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "exercise = pd.read_csv('Data/exercise.csv')\n",
    "y = np.array(exercise['Weight Lost'])\n",
    "X = exercise.drop(['Date','Weight Lost'], axis=1).values\n",
    "\n",
    "# Ordinal Encoder won't like nan values. Change to 'None'\n",
    "# This fits with data since there was 0 activity for that day\n",
    "X[:,3] = ['None' if x is np.nan else x for x in X[:,3]]\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# When putting in the columns in each imputer/encoder, indicate the column\n",
    "# of the original matrix\n",
    "  # [0]: Calories - fill missing values\n",
    "  # [1]: Exercise Type - One-hot encoding\n",
    "  # [3]: Quality of Exercise - Ordinal encoding\n",
    "\n",
    "ct = ColumnTransformer(transformers=[\n",
    "      ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean'), [0]),  # This is placed first in X\n",
    "      ('onehot', OneHotEncoder(), [1]),                                         # This is placed second in X\n",
    "      ('oe', OrdinalEncoder(categories=[['None','Low','Medium','High']]), [3])  # This is placed third in X\n",
    "    ], remainder='passthrough')                     # Remaining columns placed in order after the last encoder\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(ct.fit_transform(X))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set aside 17% of the data for testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.17, random_state=22) # , shuffle=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross validation\n",
    "\n",
    "n = 5\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "score = cross_val_score(tree_regression, X, y, scoring=\"mean_square_error\", cv=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave one out\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "score = cross_val_score(tree_regression, X, y, scoring=\"mean_square_error\", cv=len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
