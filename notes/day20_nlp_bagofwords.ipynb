{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3468ca6b",
   "metadata": {},
   "source": [
    "# The Natural Language Process  \n",
    "1. Segmentation\n",
    "    * Break the document down into sentences\n",
    "2. Tokenizing\n",
    "    * Find the individual words used\n",
    "    * Basically an array of words in the document\n",
    "3. Stop Words\n",
    "    * \"a\", \"the\", \"and\", ... commonly used words that help with sentence structure, but don't add context.\n",
    "    * Remove the stop words\n",
    "4. Stemming\n",
    "    * Different words that have the same root.\n",
    "        * Swimmer, Swimming, Swims, Swam, ...\n",
    "        * Same root of \"swim\"\n",
    "        * Tenses of words\n",
    "5. Lemmatization\n",
    "    * Some words don't stem together\n",
    "        * Ex. Universal and University are not stems of Universe\n",
    "    * Look at dictionairy definitions and associate words with common meaning\n",
    "    * \"Better\" and \"Good\" mean the same, but \"Better\" is an adjective, and \"good\" is a more general term, so we choose \"Good\" as a lemma.\n",
    "6. Speach Tagging\n",
    "    * Where is each token used in the sentence?\n",
    "    * Label each word ad Noun, Verb, Preposition, etc.\n",
    "7. Named Entity Tagging or Named Entity Recognition\n",
    "    * Flagging names of locations, movies, people, etc. that occur in the document.\n",
    "    * Is there any entity associated with a particular token?\n",
    "        * Ex. \"Utah\" might be associated with a state in the United States.\n",
    "        * Ex. \"Dusty Shaw\" might be associated with a student at Snow College."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98285366",
   "metadata": {},
   "source": [
    "# Bag of Words  \n",
    "\n",
    "Given a list of words, how likely is it to be in either bag?\n",
    "\n",
    "Create an empty array that represents 20,000 commonly used words. \n",
    "\n",
    "So we have an empty array of \n",
    "\n",
    "$$[0,0,0,0,...]$$\n",
    "\n",
    "Where each position represents a specific word.\n",
    "\n",
    "* array[0] = SOS (start of sentence)\n",
    "* array[1] = EOS (end of sentence)\n",
    "* array[n] = is reserved for any special words not represented in array  \n",
    "\n",
    "Then simply fill your array with a count of how often each word occurs in your sample.\n",
    "* Note -> Words with the same root or lemma will be counted together.  \n",
    "\n",
    "Do this with thousands of samples, and send these arrays through a machine learning model (like naive bayeds, logistic regression, decision trees, random forests, deep neural networks)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7890bba9",
   "metadata": {},
   "source": [
    "Quick example: \n",
    "\n",
    "> Fantastic work, John! Your work is worthy of JPL! Let me know if you have questions. Michael.\n",
    "\n",
    "Array:\n",
    "$$[\\text{SOS, EOS, if, is, did, not, me, you, have, get, good, perfect, worth, amazing, fantastic, job, work, ..., your, mine, ',', '.', '!', know, question, shall, go, let, ..., (special words)}]$$\n",
    "\n",
    "Your feedback produces this array:\n",
    "$$[1, 1, 1, 1, 0, 0, 1, 2, ...]$$  \n",
    "Then send this array into your model along with the 6,732 other feedbacks like it..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b429c",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1I_VzCaT_trwbAXxG34xKmQ63vVl4blAp#scrollTo=ORSDQiKsII3j "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554c3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('./Data/Restaurant_Reviews.tsv', delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0f4f32",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk' has no attribute 'internals'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m     \u001b[38;5;66;03m# For the stopwords\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstopwords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\downloader.py:774\u001b[0m, in \u001b[0;36mDownloader.download\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(s, prefix2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    766\u001b[0m     print_to(\n\u001b[0;32m    767\u001b[0m         textwrap\u001b[38;5;241m.\u001b[39mfill(\n\u001b[0;32m    768\u001b[0m             s,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    771\u001b[0m         )\n\u001b[0;32m    772\u001b[0m     )\n\u001b[1;32m--> 774\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincr_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo_or_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Error messages\u001b[39;49;00m\n\u001b[0;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mErrorMessage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\downloader.py:629\u001b[0m, in \u001b[0;36mDownloader.incr_download\u001b[1;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# Look up the requested collection or package.\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 629\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_or_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo_or_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m ErrorMessage(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo_or_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\downloader.py:603\u001b[0m, in \u001b[0;36mDownloader._info_or_id\u001b[1;34m(self, info_or_id)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_info_or_id\u001b[39m(\u001b[38;5;28mself\u001b[39m, info_or_id):\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(info_or_id, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 603\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo_or_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    605\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m info_or_id\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\downloader.py:1006\u001b[0m, in \u001b[0;36mDownloader.info\u001b[1;34m(self, id)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfo\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mid\u001b[39m):\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the ``Package`` or ``Collection`` record for the\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;124;03m    given item.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1006\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1007\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packages:\n\u001b[0;32m   1008\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packages[\u001b[38;5;28mid\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\nltk\\downloader.py:948\u001b[0m, in \u001b[0;36mDownloader._update_index\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url \u001b[38;5;241m=\u001b[39m url \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url\n\u001b[0;32m    947\u001b[0m \u001b[38;5;66;03m# Download the index file.\u001b[39;00m\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minternals\u001b[49m\u001b[38;5;241m.\u001b[39mElementWrapper(\n\u001b[0;32m    949\u001b[0m     ElementTree\u001b[38;5;241m.\u001b[39mparse(urlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url))\u001b[38;5;241m.\u001b[39mgetroot()\n\u001b[0;32m    950\u001b[0m )\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_timestamp \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# Build a dictionary of packages.\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'nltk' has no attribute 'internals'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk     # For the stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# List that will contain each cleaned review\n",
    "corpus = []\n",
    "\n",
    "# Iterate through each review, cleaning then adding to the corpus\n",
    "for i in range(0, dataset.shape[0]):\n",
    "  review = dataset['Review'][i]\n",
    "  review = re.sub(r'[^a-zA-Z!]', ' ', review)\n",
    "  review = review.lower()\n",
    "  review = review.split()\n",
    "  # Stemming\n",
    "  ps = PorterStemmer()\n",
    "  all_stopwords = stopwords.words('english')\n",
    "  all_stopwords.remove('not')\n",
    "  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
    "  review = ' '.join(review)\n",
    "  corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa854df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
