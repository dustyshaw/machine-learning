{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3468ca6b",
   "metadata": {},
   "source": [
    "# The Natural Language Process  \n",
    "1. Segmentation\n",
    "    * Break the document down into sentences\n",
    "2. Tokenizing\n",
    "    * Find the individual words used\n",
    "    * Basically an array of words in the document\n",
    "3. Stop Words\n",
    "    * \"a\", \"the\", \"and\", ... commonly used words that help with sentence structure, but don't add context.\n",
    "    * Remove the stop words\n",
    "4. Stemming\n",
    "    * Different words that have the same root.\n",
    "        * Swimmer, Swimming, Swims, Swam, ...\n",
    "        * Same root of \"swim\"\n",
    "        * Tenses of words\n",
    "5. Lemmatization\n",
    "    * Some words don't stem together\n",
    "        * Ex. Universal and University are not stems of Universe\n",
    "    * Look at dictionairy definitions and associate words with common meaning\n",
    "    * \"Better\" and \"Good\" mean the same, but \"Better\" is an adjective, and \"good\" is a more general term, so we choose \"Good\" as a lemma.\n",
    "6. Speach Tagging\n",
    "    * Where is each token used in the sentence?\n",
    "    * Label each word ad Noun, Verb, Preposition, etc.\n",
    "7. Named Entity Tagging or Named Entity Recognition\n",
    "    * Flagging names of locations, movies, people, etc. that occur in the document.\n",
    "    * Is there any entity associated with a particular token?\n",
    "        * Ex. \"Utah\" might be associated with a state in the United States.\n",
    "        * Ex. \"Dusty Shaw\" might be associated with a student at Snow College."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98285366",
   "metadata": {},
   "source": [
    "# Bag of Words  \n",
    "\n",
    "Given a list of words, how likely is it to be in either bag?\n",
    "\n",
    "Create an empty array that represents 20,000 commonly used words. \n",
    "\n",
    "So we have an empty array of \n",
    "\n",
    "$$[0,0,0,0,...]$$\n",
    "\n",
    "Where each position represents a specific word.\n",
    "\n",
    "* array[0] = SOS (start of sentence)\n",
    "* array[1] = EOS (end of sentence)\n",
    "* array[n] = is reserved for any special words not represented in array  \n",
    "\n",
    "Then simply fill your array with a count of how often each word occurs in your sample.\n",
    "* Note -> Words with the same root or lemma will be counted together.  \n",
    "\n",
    "Do this with thousands of samples, and send these arrays through a machine learning model (like naive bayeds, logistic regression, decision trees, random forests, deep neural networks)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7890bba9",
   "metadata": {},
   "source": [
    "Quick example: \n",
    "\n",
    "> Fantastic work, John! Your work is worthy of JPL! Let me know if you have questions. Michael.\n",
    "\n",
    "Array:\n",
    "$$[\\text{SOS, EOS, if, is, did, not, me, you, have, get, good, perfect, worth, amazing, fantastic, job, work, ..., your, mine, ',', '.', '!', know, question, shall, go, let, ..., (special words)}]$$\n",
    "\n",
    "Your feedback produces this array:\n",
    "$$[1, 1, 1, 1, 0, 0, 1, 2, ...]$$  \n",
    "Then send this array into your model along with the 6,732 other feedbacks like it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "554c3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('./Data/Restaurant_Reviews.tsv', delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3be5f1e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      3\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = []\n",
    "\n",
    "for i in range(0, len(data)):\n",
    "    review = data['Review'][i]\n",
    "    review = re.sub(r'[^a-zA-Z]', ' ', review)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
    "    review = \"\".join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f4f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa854df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
